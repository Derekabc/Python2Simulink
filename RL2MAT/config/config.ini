[MODEL_CONFIG]
max_grad_norm = 60
gamma = 0.95
lr_init = 5e-4
lr_decay = constant
policy = lstm
num_lstm = 64
num_h = 128
batch_size = 40
reward_norm = 100.0
reward_clip = 2.0

[TRAIN_CONFIG]
total_step = 1.5e6
test_interval = 1e4
log_interval = 10000

[ENV_CONFIG]
; coop_level is local, neighbor, global
coop_level = local
; coop discount is used to discount the neighbors' impact
coop_gamma = 0.7
data_path = ./large_grid/data/
episode_length = 765
; objective is used to choose different reward functions
objective = max_flow
seed = 42
test_seeds = 10000,20000,30000
